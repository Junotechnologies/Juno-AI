{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e60899a",
   "metadata": {},
   "source": [
    "## üîß 1. Environment Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e910d3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Starting imports...\n",
      "‚è±Ô∏è Basic libraries imported: 1.62s\n",
      "üîÑ Importing BigQuery & Vertex AI...\n",
      "‚è±Ô∏è Basic libraries imported: 1.62s\n",
      "üîÑ Importing BigQuery & Vertex AI...\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(\"üîÑ Starting imports...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Set environment to use gcloud auth (remove empty GOOGLE_APPLICATION_CREDENTIALS)\n",
    "if 'GOOGLE_APPLICATION_CREDENTIALS' in os.environ and os.environ['GOOGLE_APPLICATION_CREDENTIALS'] == '':\n",
    "    del os.environ['GOOGLE_APPLICATION_CREDENTIALS']\n",
    "os.environ['GCLOUD_PROJECT'] = 'junoplus-dev'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(f\"‚è±Ô∏è Basic libraries imported: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "# BigQuery & Vertex AI (import directly without immediate auth)\n",
    "print(\"üîÑ Importing BigQuery & Vertex AI...\")\n",
    "start = time.time()\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "print(f\"‚è±Ô∏è Google Cloud libraries imported: {time.time() - start:.2f}s\")\n",
    "\n",
    "# Deep Learning libraries\n",
    "print(\"üîÑ Importing TensorFlow (this may take a while)...\")\n",
    "start = time.time()\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "print(f\"‚è±Ô∏è TensorFlow imported: {time.time() - start:.2f}s\")\n",
    "\n",
    "# Scikit-learn utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow logging\n",
    "\n",
    "# Configure display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Set up project configuration\n",
    "PROJECT_ID = 'junoplus-dev'\n",
    "REGION = 'us-central1'\n",
    "DATASET_ID = 'junoplus_analytics'\n",
    "\n",
    "# Initialize BigQuery client (actual connection happens here)\n",
    "print(\"üîÑ Initializing BigQuery client...\")\n",
    "start = time.time()\n",
    "client = bigquery.Client(project=PROJECT_ID, location=REGION)\n",
    "print(f\"‚è±Ô∏è BigQuery client initialized: {time.time() - start:.2f}s\")\n",
    "\n",
    "print(\"üîÑ Initializing Vertex AI...\")\n",
    "start = time.time()\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "print(f\"‚è±Ô∏è Vertex AI initialized: {time.time() - start:.2f}s\")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# GPU check\n",
    "print(\"üîÑ Detecting GPU...\")\n",
    "start = time.time()\n",
    "gpu_available = len(tf.config.list_physical_devices('GPU')) > 0\n",
    "print(f\"‚è±Ô∏è GPU detection completed: {time.time() - start:.2f}s\")\n",
    "\n",
    "print(f\"\\n‚úÖ Environment setup complete!\")\n",
    "print(f\"üìä Project: {PROJECT_ID}\")\n",
    "print(f\"üåç Region: {REGION}\")\n",
    "print(f\"üíæ Dataset: {DATASET_ID}\")\n",
    "print(f\"üî¢ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"üéÆ GPU Available: {gpu_available}\")\n",
    "print(f\"‚è±Ô∏è Total setup time: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ffeb52",
   "metadata": {},
   "source": [
    "## üìä 2. Data Loading & Feature Engineering\n",
    "\n",
    "### Target Variables (from mostUsedSettings):\n",
    "- **y_heat**: Heat Level (0-3, 4 classes)\n",
    "- **y_mode**: TENS Mode (0-3, 4 classes)\n",
    "- **y_tens**: TENS Level (0-10, 11 classes)\n",
    "\n",
    "All targets will be **one-hot encoded** for neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12577080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define medication potency mapping (from synthetic data generator)\n",
    "MEDICATION_POTENCY = {\n",
    "    'Advil': 1.15,\n",
    "    'Midol': 1.0,\n",
    "    'Naproxen': 1.3,\n",
    "    'Paracetamol': 0.9,\n",
    "    'Ibuprofen': 1.1,\n",
    "    'Voltaren': 1.05,\n",
    "    'Cycle Support Supplement': 0.5,\n",
    "    'Vitamin D': 0.3,\n",
    "    'Birth Control Pill': 0.4,\n",
    "    'Lo Loestrin FE': 0.45,\n",
    "    'Drospirenone-EE': 0.5,\n",
    "}\n",
    "\n",
    "# List of hormonal medications\n",
    "HORMONAL_MEDICATIONS = [\n",
    "    'Birth Control Pill',\n",
    "    'Lo Loestrin FE',\n",
    "    'Drospirenone-EE'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdebecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading data from BigQuery...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 121\u001b[39m\n\u001b[32m      3\u001b[39m query = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33mWITH \u001b[39m\n\u001b[32m      5\u001b[39m \u001b[33m-- Extract device size from device name\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33mSELECT * FROM main_data\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müîÑ Loading data from BigQuery...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m df = \u001b[43mclient\u001b[49m.query(query).to_dataframe()\n\u001b[32m    122\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Loaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sessions from BigQuery\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    123\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìä Data split distribution:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "# Load data from BigQuery with comprehensive feature engineering\n",
    "# Same query as Approach 2 for consistency\n",
    "query = \"\"\"\n",
    "WITH \n",
    "-- Extract device size from device name\n",
    "device_info AS (\n",
    "  SELECT \n",
    "    sessionId,\n",
    "    CASE \n",
    "      WHEN LOWER(deviceName) LIKE '%grand%' THEN 'Grand'\n",
    "      WHEN LOWER(deviceName) LIKE '%petit%' THEN 'Petit'\n",
    "      ELSE 'Unknown'\n",
    "    END AS device_size\n",
    "  FROM `junoplus-dev.junoplus_analytics.ml_training_data`\n",
    "),\n",
    "\n",
    "-- Calculate user-level historical preferences\n",
    "user_history AS (\n",
    "  SELECT \n",
    "    userId,\n",
    "    AVG(target_heat_level) AS user_avg_heat,\n",
    "    AVG(target_tens_mode) AS user_avg_mode,\n",
    "    AVG(target_tens_level) AS user_avg_tens,\n",
    "    APPROX_TOP_COUNT(target_heat_level, 1)[OFFSET(0)].value AS user_mode_heat,\n",
    "    APPROX_TOP_COUNT(target_tens_mode, 1)[OFFSET(0)].value AS user_mode_mode,\n",
    "    APPROX_TOP_COUNT(target_tens_level, 1)[OFFSET(0)].value AS user_mode_tens\n",
    "  FROM `junoplus-dev.junoplus_analytics.ml_training_data`\n",
    "  WHERE target_heat_level IS NOT NULL\n",
    "    AND target_tens_level IS NOT NULL\n",
    "    AND target_tens_mode IS NOT NULL\n",
    "  GROUP BY userId\n",
    "),\n",
    "\n",
    "-- Main data with all features\n",
    "main_data AS (\n",
    "  SELECT \n",
    "    t.sessionId,\n",
    "    t.userId AS user_id,\n",
    "    t.therapyStartTime,\n",
    "    \n",
    "    -- TARGET VARIABLES\n",
    "    target_heat_level AS y_heat,\n",
    "    target_tens_mode AS y_mode,\n",
    "    target_tens_level AS y_tens,\n",
    "    \n",
    "    -- ADJUSTMENT DELTA FEATURES\n",
    "    (final_heat_level - target_heat_level) AS delta_heat,\n",
    "    (final_tens_level - target_tens_level) AS delta_tens,\n",
    "    (final_tens_mode - target_tens_mode) AS delta_mode,\n",
    "    \n",
    "    -- CYCLE CONTEXT FEATURES\n",
    "    COALESCE(cycle_day, 15) AS days_since_period_start,\n",
    "    is_period_day AS is_near_period,\n",
    "    cycle_phase_estimated,\n",
    "    period_pain_level,\n",
    "    flow_level,\n",
    "    \n",
    "    -- MEDICATION CONTEXT FEATURES\n",
    "    has_pain_medication,\n",
    "    medication_count,\n",
    "    active_medication_count,\n",
    "    recent_medication_usage,\n",
    "    pain_medication_adherence,\n",
    "    \n",
    "    -- USER CONTEXT\n",
    "    age,\n",
    "    age_group,\n",
    "    cycle_length,\n",
    "    period_length,\n",
    "    days_since_signup,\n",
    "    user_experience_level,\n",
    "    \n",
    "    -- SESSION CONTEXT\n",
    "    session_hour,\n",
    "    day_of_week,\n",
    "    time_of_day_category,\n",
    "    therapyDuration,\n",
    "    \n",
    "    -- PAIN & EFFECTIVENESS\n",
    "    input_pain_level,\n",
    "    pain_level_before,\n",
    "    pain_level_after,\n",
    "    pain_reduction,\n",
    "    pain_reduction_percentage,\n",
    "    was_effective,\n",
    "    \n",
    "    -- DEVICE INFO\n",
    "    d.device_size,\n",
    "    most_used_battery_level,\n",
    "    \n",
    "    -- USER HISTORICAL PREFERENCES\n",
    "    h.user_avg_heat,\n",
    "    h.user_avg_mode,\n",
    "    h.user_avg_tens,\n",
    "    h.user_mode_heat,\n",
    "    h.user_mode_mode,\n",
    "    h.user_mode_tens,\n",
    "    \n",
    "    -- DATA SPLIT (user-stable split)\n",
    "    CASE \n",
    "      WHEN MOD(FARM_FINGERPRINT(t.userId), 10) < 7 THEN 'TRAIN'\n",
    "      WHEN MOD(FARM_FINGERPRINT(t.userId), 10) < 9 THEN 'EVAL'\n",
    "      ELSE 'TEST'\n",
    "    END AS data_split\n",
    "    \n",
    "  FROM `junoplus-dev.junoplus_analytics.ml_training_data` t\n",
    "  LEFT JOIN device_info d ON t.sessionId = d.sessionId\n",
    "  LEFT JOIN user_history h ON t.userId = h.userId\n",
    "  \n",
    "  WHERE target_heat_level IS NOT NULL\n",
    "    AND target_tens_level IS NOT NULL\n",
    "    AND target_tens_mode IS NOT NULL\n",
    "    AND session_quality = 'high_quality'\n",
    "    AND user_made_adjustments = TRUE\n",
    ")\n",
    "\n",
    "SELECT * FROM main_data\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîÑ Loading data from BigQuery...\")\n",
    "df = client.query(query).to_dataframe()\n",
    "\n",
    "# Rename user_id back to userId for compatibility\n",
    "df.rename(columns={'user_id': 'userId'}, inplace=True)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df):,} sessions from BigQuery\")\n",
    "print(f\"\\nüìä Data split distribution:\")\n",
    "print(df['data_split'].value_counts())\n",
    "print(f\"\\nüë• Unique users: {df['userId'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c02c42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data overview\n",
    "print(\"\\nüìã Dataset Overview:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nüìä Target Variable Distributions:\")\n",
    "print(\"\\nHeat Level (y_heat):\")\n",
    "print(df['y_heat'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nTENS Mode (y_mode):\")\n",
    "print(df['y_mode'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nTENS Level (y_tens):\")\n",
    "print(df['y_tens'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6b7b34",
   "metadata": {},
   "source": [
    "## üîß 3. Data Preprocessing & Feature Scaling\n",
    "\n",
    "### Critical for Neural Networks:\n",
    "1. **Feature Scaling**: Standardize all continuous features (mean=0, std=1)\n",
    "2. **One-Hot Encoding**: Convert categorical features to binary columns\n",
    "3. **Target Encoding**: Convert targets to one-hot encoded vectors\n",
    "4. **Train/Eval/Test Split**: Maintain user-stable split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f965abf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "print(\"üîß Preprocessing features...\")\n",
    "\n",
    "# Fill missing numerical values with median\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_cols = [col for col in numerical_cols if col not in \n",
    "                  ['y_heat', 'y_mode', 'y_tens', 'sessionId', 'userId']]\n",
    "\n",
    "for col in numerical_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "# Fill missing categorical values\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "categorical_cols = [col for col in categorical_cols if col not in \n",
    "                    ['sessionId', 'userId', 'data_split', 'therapyStartTime']]\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna('Unknown', inplace=True)\n",
    "\n",
    "print(f\"‚úÖ Missing values handled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec4d3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "print(\"üî¢ Encoding categorical variables...\")\n",
    "\n",
    "encode_cols = ['device_size', 'time_of_day_category', 'cycle_phase_estimated', \n",
    "               'age_group', 'user_experience_level']\n",
    "\n",
    "df_encoded = pd.get_dummies(df, columns=encode_cols, drop_first=True, dtype=int)\n",
    "\n",
    "print(f\"‚úÖ Encoded {len(encode_cols)} categorical features\")\n",
    "print(f\"üìä Total features after encoding: {len(df_encoded.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825a1c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns\n",
    "exclude_cols = ['sessionId', 'userId', 'therapyStartTime', 'data_split',\n",
    "                'y_heat', 'y_mode', 'y_tens']\n",
    "\n",
    "feature_cols = [col for col in df_encoded.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"\\n‚úÖ Feature columns: {len(feature_cols)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd123d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data by user (user-stable split)\n",
    "print(\"\\nüìä Splitting data into TRAIN, EVAL, TEST sets...\")\n",
    "\n",
    "train_df = df_encoded[df_encoded['data_split'] == 'TRAIN'].copy()\n",
    "eval_df = df_encoded[df_encoded['data_split'] == 'EVAL'].copy()\n",
    "test_df = df_encoded[df_encoded['data_split'] == 'TEST'].copy()\n",
    "\n",
    "print(f\"\\n‚úÖ Data split complete:\")\n",
    "print(f\"   TRAIN: {len(train_df):,} sessions\")\n",
    "print(f\"   EVAL:  {len(eval_df):,} sessions\")\n",
    "print(f\"   TEST:  {len(test_df):,} sessions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae5688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and targets\n",
    "print(\"\\nüéØ Extracting features and targets...\")\n",
    "\n",
    "X_train = train_df[feature_cols].values\n",
    "X_eval = eval_df[feature_cols].values\n",
    "X_test = test_df[feature_cols].values\n",
    "\n",
    "# Extract target variables (will be one-hot encoded later)\n",
    "y_train_heat = train_df['y_heat'].values\n",
    "y_train_mode = train_df['y_mode'].values\n",
    "y_train_tens = train_df['y_tens'].values\n",
    "\n",
    "y_eval_heat = eval_df['y_heat'].values\n",
    "y_eval_mode = eval_df['y_mode'].values\n",
    "y_eval_tens = eval_df['y_tens'].values\n",
    "\n",
    "y_test_heat = test_df['y_heat'].values\n",
    "y_test_mode = test_df['y_mode'].values\n",
    "y_test_tens = test_df['y_tens'].values\n",
    "\n",
    "print(f\"‚úÖ Features extracted: {X_train.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdbdf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL: Scale features for neural network\n",
    "print(\"\\n‚öñÔ∏è Scaling features (StandardScaler)...\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_eval_scaled = scaler.transform(X_eval)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"‚úÖ Features scaled successfully\")\n",
    "print(f\"   Mean: {X_train_scaled.mean():.6f} (should be ‚âà 0)\")\n",
    "print(f\"   Std:  {X_train_scaled.std():.6f} (should be ‚âà 1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98493e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode target variables\n",
    "print(\"\\nüéØ One-hot encoding target variables...\")\n",
    "\n",
    "# Heat Level: 4 classes (0, 1, 2, 3)\n",
    "y_train_heat_onehot = to_categorical(y_train_heat, num_classes=4)\n",
    "y_eval_heat_onehot = to_categorical(y_eval_heat, num_classes=4)\n",
    "y_test_heat_onehot = to_categorical(y_test_heat, num_classes=4)\n",
    "\n",
    "# TENS Mode: 4 classes (0, 1, 2, 3)\n",
    "y_train_mode_onehot = to_categorical(y_train_mode, num_classes=4)\n",
    "y_eval_mode_onehot = to_categorical(y_eval_mode, num_classes=4)\n",
    "y_test_mode_onehot = to_categorical(y_test_mode, num_classes=4)\n",
    "\n",
    "# TENS Level: 11 classes (0, 1, 2, ..., 10)\n",
    "y_train_tens_onehot = to_categorical(y_train_tens, num_classes=11)\n",
    "y_eval_tens_onehot = to_categorical(y_eval_tens, num_classes=11)\n",
    "y_test_tens_onehot = to_categorical(y_test_tens, num_classes=11)\n",
    "\n",
    "print(f\"‚úÖ Targets one-hot encoded:\")\n",
    "print(f\"   Heat: {y_train_heat_onehot.shape[1]} classes\")\n",
    "print(f\"   Mode: {y_train_mode_onehot.shape[1]} classes\")\n",
    "print(f\"   Tens: {y_train_tens_onehot.shape[1]} classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4f3eb9",
   "metadata": {},
   "source": [
    "## üèóÔ∏è 4. Build Multi-Output Neural Network\n",
    "\n",
    "### Architecture:\n",
    "```\n",
    "Input Layer (n_features)\n",
    "    ‚Üì\n",
    "Dense(256) + ReLU + Dropout(0.3)\n",
    "    ‚Üì\n",
    "Dense(128) + ReLU + Dropout(0.3)\n",
    "    ‚Üì\n",
    "Dense(64) + ReLU + Dropout(0.2)\n",
    "    ‚Üì\n",
    "    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚Üì             ‚Üì             ‚Üì\n",
    "Heat Head     Mode Head    Level Head\n",
    "Dense(4)      Dense(4)     Dense(11)\n",
    "Softmax       Softmax      Softmax\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954258c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build multi-output neural network\n",
    "print(\"üèóÔ∏è Building multi-output neural network...\")\n",
    "\n",
    "# Input layer\n",
    "input_layer = layers.Input(shape=(X_train_scaled.shape[1],), name='input')\n",
    "\n",
    "# Shared base network (feature extraction)\n",
    "x = layers.Dense(256, activation='relu', name='shared_dense_1')(input_layer)\n",
    "x = layers.BatchNormalization(name='batch_norm_1')(x)\n",
    "x = layers.Dropout(0.3, name='dropout_1')(x)\n",
    "\n",
    "x = layers.Dense(128, activation='relu', name='shared_dense_2')(x)\n",
    "x = layers.BatchNormalization(name='batch_norm_2')(x)\n",
    "x = layers.Dropout(0.3, name='dropout_2')(x)\n",
    "\n",
    "x = layers.Dense(64, activation='relu', name='shared_dense_3')(x)\n",
    "x = layers.BatchNormalization(name='batch_norm_3')(x)\n",
    "x = layers.Dropout(0.2, name='dropout_3')(x)\n",
    "\n",
    "# Output heads (task-specific branches)\n",
    "\n",
    "# Heat Level Head (4 classes)\n",
    "heat_branch = layers.Dense(32, activation='relu', name='heat_dense')(x)\n",
    "heat_output = layers.Dense(4, activation='softmax', name='heat_output')(heat_branch)\n",
    "\n",
    "# TENS Mode Head (4 classes)\n",
    "mode_branch = layers.Dense(32, activation='relu', name='mode_dense')(x)\n",
    "mode_output = layers.Dense(4, activation='softmax', name='mode_output')(mode_branch)\n",
    "\n",
    "# TENS Level Head (11 classes)\n",
    "tens_branch = layers.Dense(32, activation='relu', name='tens_dense')(x)\n",
    "tens_output = layers.Dense(11, activation='softmax', name='tens_output')(tens_branch)\n",
    "\n",
    "# Create model\n",
    "model = models.Model(\n",
    "    inputs=input_layer,\n",
    "    outputs=[heat_output, mode_output, tens_output],\n",
    "    name='MultiOutput_DeviceSettings'\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model architecture created!\")\n",
    "print(f\"\\nüìä Model Summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251cbda1",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 5. Compile Model with Multi-Loss Configuration\n",
    "\n",
    "### Loss Function:\n",
    "- **Categorical Cross-Entropy** for each output head\n",
    "- **Total Loss** = Heat Loss + Mode Loss + Level Loss\n",
    "\n",
    "### Optimizer:\n",
    "- **Adam** with learning rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff2f1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model with multiple losses\n",
    "print(\"‚öôÔ∏è Compiling model...\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss={\n",
    "        'heat_output': 'categorical_crossentropy',\n",
    "        'mode_output': 'categorical_crossentropy',\n",
    "        'tens_output': 'categorical_crossentropy'\n",
    "    },\n",
    "    loss_weights={\n",
    "        'heat_output': 1.0,\n",
    "        'mode_output': 1.0,\n",
    "        'tens_output': 1.0\n",
    "    },\n",
    "    metrics={\n",
    "        'heat_output': ['accuracy'],\n",
    "        'mode_output': ['accuracy'],\n",
    "        'tens_output': ['accuracy']\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model compiled successfully!\")\n",
    "print(\"\\nüìã Configuration:\")\n",
    "print(\"   Optimizer: Adam (lr=0.001)\")\n",
    "print(\"   Loss: Categorical Cross-Entropy (3 heads)\")\n",
    "print(\"   Metrics: Accuracy (per head)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33c54f2",
   "metadata": {},
   "source": [
    "## üöÄ 6. Train the Multi-Output Model\n",
    "\n",
    "### Training Configuration:\n",
    "- **Epochs**: 50\n",
    "- **Batch Size**: 64\n",
    "- **Callbacks**: Early Stopping, Model Checkpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd931e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up callbacks\n",
    "print(\"üîß Setting up training callbacks...\")\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('models/multioutput_approach', exist_ok=True)\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save best model\n",
    "model_checkpoint = callbacks.ModelCheckpoint(\n",
    "    'models/multioutput_approach/best_model.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Reduce learning rate when stuck\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Callbacks configured:\")\n",
    "print(\"   ‚Ä¢ Early Stopping (patience=10)\")\n",
    "print(\"   ‚Ä¢ Model Checkpoint (save best)\")\n",
    "print(\"   ‚Ä¢ ReduceLROnPlateau (factor=0.5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd2eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"\\nüöÄ Training multi-output neural network...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled,\n",
    "    {\n",
    "        'heat_output': y_train_heat_onehot,\n",
    "        'mode_output': y_train_mode_onehot,\n",
    "        'tens_output': y_train_tens_onehot\n",
    "    },\n",
    "    validation_data=(\n",
    "        X_eval_scaled,\n",
    "        {\n",
    "            'heat_output': y_eval_heat_onehot,\n",
    "            'mode_output': y_eval_mode_onehot,\n",
    "            'tens_output': y_eval_tens_onehot\n",
    "        }\n",
    "    ),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdf7fb0",
   "metadata": {},
   "source": [
    "## üìä 7. Training History Visualization\n",
    "\n",
    "Visualize loss and accuracy curves for all three output heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bcdfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "print(\"üìä Visualizing training history...\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Total Loss\n",
    "axes[0, 0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "axes[0, 0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0, 0].set_title('Total Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Heat Level Accuracy\n",
    "axes[0, 1].plot(history.history['heat_output_accuracy'], label='Train Acc', linewidth=2)\n",
    "axes[0, 1].plot(history.history['val_heat_output_accuracy'], label='Val Acc', linewidth=2)\n",
    "axes[0, 1].set_title('Heat Level Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# TENS Mode Accuracy\n",
    "axes[1, 0].plot(history.history['mode_output_accuracy'], label='Train Acc', linewidth=2)\n",
    "axes[1, 0].plot(history.history['val_mode_output_accuracy'], label='Val Acc', linewidth=2)\n",
    "axes[1, 0].set_title('TENS Mode Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# TENS Level Accuracy\n",
    "axes[1, 1].plot(history.history['tens_output_accuracy'], label='Train Acc', linewidth=2)\n",
    "axes[1, 1].plot(history.history['val_tens_output_accuracy'], label='Val Acc', linewidth=2)\n",
    "axes[1, 1].set_title('TENS Level Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Accuracy')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Training history visualized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca3eb61",
   "metadata": {},
   "source": [
    "## üîÆ 8. Prediction & Evaluation on Test Set\n",
    "\n",
    "Generate predictions for all three outputs and evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06940cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "print(\"üîÆ Generating predictions on test set...\")\n",
    "\n",
    "# Get probability predictions for all three heads\n",
    "predictions = model.predict(X_test_scaled, verbose=0)\n",
    "y_pred_heat_proba, y_pred_mode_proba, y_pred_tens_proba = predictions\n",
    "\n",
    "# Convert probabilities to class predictions using argmax\n",
    "y_pred_heat = np.argmax(y_pred_heat_proba, axis=1)\n",
    "y_pred_mode = np.argmax(y_pred_mode_proba, axis=1)\n",
    "y_pred_tens = np.argmax(y_pred_tens_proba, axis=1)\n",
    "\n",
    "print(f\"‚úÖ Predictions generated for {len(y_pred_heat):,} test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69170b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics\n",
    "print(\"\\nüìä MULTI-OUTPUT DEEP LEARNING MODEL EVALUATION\\n\" + \"=\"*80)\n",
    "\n",
    "# Heat Level\n",
    "heat_accuracy = accuracy_score(y_test_heat, y_pred_heat)\n",
    "heat_f1 = f1_score(y_test_heat, y_pred_heat, average='weighted')\n",
    "\n",
    "print(\"\\nüî• HEAT LEVEL PREDICTION:\")\n",
    "print(f\"   Accuracy: {heat_accuracy:.4f}\")\n",
    "print(f\"   F1 Score: {heat_f1:.4f}\")\n",
    "print(\"\\n   Classification Report:\")\n",
    "print(classification_report(y_test_heat, y_pred_heat, \n",
    "                           target_names=['Heat 0', 'Heat 1', 'Heat 2', 'Heat 3']))\n",
    "\n",
    "# TENS Mode\n",
    "mode_accuracy = accuracy_score(y_test_mode, y_pred_mode)\n",
    "mode_f1 = f1_score(y_test_mode, y_pred_mode, average='weighted')\n",
    "\n",
    "print(\"\\n‚ö° TENS MODE PREDICTION:\")\n",
    "print(f\"   Accuracy: {mode_accuracy:.4f}\")\n",
    "print(f\"   F1 Score: {mode_f1:.4f}\")\n",
    "print(\"\\n   Classification Report:\")\n",
    "print(classification_report(y_test_mode, y_pred_mode,\n",
    "                           target_names=['Mode 0', 'Mode 1', 'Mode 2', 'Mode 3']))\n",
    "\n",
    "# TENS Level\n",
    "tens_accuracy = accuracy_score(y_test_tens, y_pred_tens)\n",
    "tens_f1 = f1_score(y_test_tens, y_pred_tens, average='weighted')\n",
    "\n",
    "print(\"\\nüéØ TENS LEVEL PREDICTION:\")\n",
    "print(f\"   Accuracy: {tens_accuracy:.4f}\")\n",
    "print(f\"   F1 Score: {tens_f1:.4f}\")\n",
    "print(\"\\n   Classification Report:\")\n",
    "print(classification_report(y_test_tens, y_pred_tens,\n",
    "                           target_names=[f'Level {i}' for i in range(11)],\n",
    "                           zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf9d7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrices\n",
    "print(\"\\nüìà Generating confusion matrices...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Heat Level Confusion Matrix\n",
    "cm_heat = confusion_matrix(y_test_heat, y_pred_heat)\n",
    "sns.heatmap(cm_heat, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=[0, 1, 2, 3], yticklabels=[0, 1, 2, 3])\n",
    "axes[0].set_title(f'Heat Level Confusion Matrix\\nAccuracy: {heat_accuracy:.3f}')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "# TENS Mode Confusion Matrix\n",
    "cm_mode = confusion_matrix(y_test_mode, y_pred_mode)\n",
    "sns.heatmap(cm_mode, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
    "            xticklabels=[0, 1, 2, 3], yticklabels=[0, 1, 2, 3])\n",
    "axes[1].set_title(f'TENS Mode Confusion Matrix\\nAccuracy: {mode_accuracy:.3f}')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "# TENS Level Confusion Matrix (showing only levels 0-5 for visibility)\n",
    "cm_tens = confusion_matrix(y_test_tens, y_pred_tens)\n",
    "cm_tens_display = cm_tens[:6, :6]\n",
    "sns.heatmap(cm_tens_display, annot=True, fmt='d', cmap='Oranges', ax=axes[2],\n",
    "            xticklabels=range(6), yticklabels=range(6))\n",
    "axes[2].set_title(f'TENS Level Confusion Matrix (0-5)\\nAccuracy: {tens_accuracy:.3f}')\n",
    "axes[2].set_xlabel('Predicted')\n",
    "axes[2].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Confusion matrices visualized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70da7b5c",
   "metadata": {},
   "source": [
    "## üìä 9. Final Summary & Comparison\n",
    "\n",
    "Comprehensive summary of the multi-output deep learning approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012faf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ MULTI-OUTPUT DEEP LEARNING APPROACH - FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    'Output Head': ['Heat Level', 'TENS Mode', 'TENS Level'],\n",
    "    'Classes': [4, 4, 11],\n",
    "    'Accuracy': [heat_accuracy, mode_accuracy, tens_accuracy],\n",
    "    'F1 Score': [heat_f1, mode_f1, tens_f1]\n",
    "})\n",
    "\n",
    "print(\"\\n\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_accuracy = summary_df['Accuracy'].mean()\n",
    "avg_f1 = summary_df['F1 Score'].mean()\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(f\"üìä OVERALL PERFORMANCE:\")\n",
    "print(f\"   Average Accuracy: {avg_accuracy:.4f}\")\n",
    "print(f\"   Average F1 Score: {avg_f1:.4f}\")\n",
    "\n",
    "# Model statistics\n",
    "total_params = model.count_params()\n",
    "print(f\"\\nüîß MODEL STATISTICS:\")\n",
    "print(f\"   Total Parameters: {total_params:,}\")\n",
    "print(f\"   Training Samples: {len(X_train):,}\")\n",
    "print(f\"   Evaluation Samples: {len(X_eval):,}\")\n",
    "print(f\"   Test Samples: {len(X_test):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\n‚úÖ Approach 3 (Multi-Output Deep Learning) Complete!\")\n",
    "print(\"\\nüìå Key Advantages:\")\n",
    "print(\"   ‚Ä¢ Single unified model for all predictions\")\n",
    "print(\"   ‚Ä¢ Shared feature learning across tasks\")\n",
    "print(\"   ‚Ä¢ Parameter efficient architecture\")\n",
    "print(\"   ‚Ä¢ Captures inter-dependencies between settings\")\n",
    "print(\"   ‚Ä¢ Production-ready for deployment\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b3f86b",
   "metadata": {},
   "source": [
    "## üíæ 10. Model Export & Deployment\n",
    "\n",
    "Save the trained model and preprocessing artifacts for production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8236040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and preprocessing artifacts\n",
    "import joblib\n",
    "\n",
    "print(\"üíæ Saving model and artifacts...\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save('models/multioutput_approach/final_model.h5')\n",
    "print(\"‚úÖ Model saved: models/multioutput_approach/final_model.h5\")\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, 'models/multioutput_approach/feature_scaler.pkl')\n",
    "print(\"‚úÖ Scaler saved: models/multioutput_approach/feature_scaler.pkl\")\n",
    "\n",
    "# Save feature names\n",
    "joblib.dump(feature_cols, 'models/multioutput_approach/feature_columns.pkl')\n",
    "print(\"‚úÖ Feature columns saved: models/multioutput_approach/feature_columns.pkl\")\n",
    "\n",
    "# Save training history\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv('models/multioutput_approach/training_history.csv', index=False)\n",
    "print(\"‚úÖ Training history saved: models/multioutput_approach/training_history.csv\")\n",
    "\n",
    "print(\"\\nüì¶ All artifacts saved successfully!\")\n",
    "print(\"\\nüöÄ Ready for deployment to Vertex AI!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a87f246",
   "metadata": {},
   "source": [
    "## üîç 11. Prediction Example (Optional)\n",
    "\n",
    "Demonstrate how to use the trained model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23527083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Make prediction for a single sample\n",
    "print(\"üîç Example Prediction:\\n\")\n",
    "\n",
    "# Take first test sample\n",
    "sample_idx = 0\n",
    "sample_features = X_test_scaled[sample_idx:sample_idx+1]\n",
    "\n",
    "# Make prediction\n",
    "sample_pred = model.predict(sample_features, verbose=0)\n",
    "pred_heat_proba, pred_mode_proba, pred_tens_proba = sample_pred\n",
    "\n",
    "# Get class predictions\n",
    "pred_heat = np.argmax(pred_heat_proba)\n",
    "pred_mode = np.argmax(pred_mode_proba)\n",
    "pred_tens = np.argmax(pred_tens_proba)\n",
    "\n",
    "# Get actual values\n",
    "actual_heat = y_test_heat[sample_idx]\n",
    "actual_mode = y_test_mode[sample_idx]\n",
    "actual_tens = y_test_tens[sample_idx]\n",
    "\n",
    "print(f\"Sample Index: {sample_idx}\")\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(f\"{'Setting':<20} {'Predicted':<15} {'Actual':<15} {'Match'}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Heat Level':<20} {pred_heat:<15} {actual_heat:<15} {'‚úÖ' if pred_heat == actual_heat else '‚ùå'}\")\n",
    "print(f\"{'TENS Mode':<20} {pred_mode:<15} {actual_mode:<15} {'‚úÖ' if pred_mode == actual_mode else '‚ùå'}\")\n",
    "print(f\"{'TENS Level':<20} {pred_tens:<15} {actual_tens:<15} {'‚úÖ' if pred_tens == actual_tens else '‚ùå'}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "print(\"\\nüìä Prediction Confidence:\")\n",
    "print(f\"   Heat Level: {pred_heat_proba[0][pred_heat]:.2%}\")\n",
    "print(f\"   TENS Mode:  {pred_mode_proba[0][pred_mode]:.2%}\")\n",
    "print(f\"   TENS Level: {pred_tens_proba[0][pred_tens]:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
